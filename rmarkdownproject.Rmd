
---
title: "Project Assignment on Practical Machine Learning"
author: "Nayan Pandya"
date: "December 1, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Project: Practical machine learning

### Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset)


## Data
The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.


the content summary of weight lifting dataset:
reference from http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises 

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. We made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg)."

## Acknowledgement

Data is from: Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H.
Qualitative Activity Recognition of Weight Lifting Exercises. 
Proceedings of 4th International Conference in Cooperation with SIGCHI 
(Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

## step by step procedure to built prediction 

1. loading data from testing and training data set
2. remove unwanted column where we have less than 60% records.
3. check multi-colinearity
4. use VIF function to remove colinear variable
5. check accuracy of three types of model and use best method to predict testing data set.
## Read library to use packages for programme
```{r}

library(rpart)
library(caret)
library(ROSE)
library(Hmisc)
library(gridExtra)
library(GGally)
library(dummies)
library(car)
library(rpart.plot)
library(rattle)
library(randomForest)
library(knitr)

set.seed(3546)

```

# Read file from location

```{r}
# setting working directory where traing and testing dataset stored
setwd("C:/D drive/Digital strategy/field data/coursera/practical machinelearning/projectwork")
rm(list=ls())
# it is showing number of file exist in above listed folder path
dir()
# Following command is reading .csv file from folder given above
testing <- read.csv("pml-testing.csv")
training <- read.csv("pml-training.csv")

```
# remove with NA columns more than 60%
```{r}
# it is very important to clean data by replacing NA or removing particular column. Here, we have more than 60% NA in many of column. So we will be removing this column by writing following command. 

training <- training[,-which(colSums(is.na(training))>(0.6*length(training$classe)))]

```
# Partition of data into training and test set

```{r}
# We need to split training data into train and test data set. We are splitting data such that 80% records will be going into training data set while 20% data will be going into test dataset. 

intrain <- createDataPartition(y=training$classe,p=0.8,list = FALSE)
train_data<- training[intrain,]
test_data <- training[-intrain,]

```
# Remove near zero variable from train and test data

```{r}
# We are reducing data by removing those columns which has zero variance column. zero variance factor doesn't impact on output   at all. We have splitted data previously, so we will apply treatment for both train and test dataset.    
nsv <- nearZeroVar(train_data,saveMetrics = TRUE)
train_data <- train_data[,-c(which(nsv$nzv== TRUE))]
rm(nsv)
nsv <- nearZeroVar(test_data,saveMetrics = TRUE)
test_data <- test_data[,-c(which(nsv$nzv== TRUE))]

``` 
# Convert all independent variable into numetic
```{r}
# Next step, we will be checking multicolinearity so we are converting intiger and factor to numeric vactor to run regression model. Following code is converting complete dataset except predictor variable into numberic.

train_num <- data.frame(lapply(train_data[,-59], function(x) as.numeric((x))))
train_data <- data.frame(train_num,train_data$classe)
test_num <- data.frame(lapply(test_data[,-59], function(x) as.numeric((x))))
test_data <- data.frame(test_num,test_data$classe)

```
# remove identification variable
``` {r}
# There are some idepntification variable which will create trouble while running prediction algorith, so we are removing those from test and train data set.  
train_data <- train_data[,-c(1:5)]
test_data <- test_data[,-c(1:5)]
```

## check multicolineaty
```{r}

# Our response variable is catagorical variable so we should choose any independent variable as response variable and check multicolinearity

# Check VIF factor and remove variable which has VIF more than 5 but need to check iteratively. Based on multiple itertive run, it was found out that following variables are having multicolinearity so we are removing the same.
train_vif <- train_data[,-c(3,4,5,9,10,11,12,20,22,23,26,27,34,35,36,37,38,47,48,49)]

# Train_vif is our final train data set which will be used for building predictive model. it has 34 column and 15699 row  

str(train_vif)
dim(train_vif)
## same treatment was applied for test_vif so that nuber of column and column name are unique with train_vif data set.
test_vif <- test_data[,-c(3,4,5,9,10,11,12,20,22,23,26,27,34,35,36,37,38,47,48,49)]
# test_vif is our final test dataset for calculating out of sample error
dim(test_vif)

```
# Prediction with using Random Forest 

```{r}
# There are two option to run random forest. 1. use Train fuciton under CARET and 2. call random forest library. we have used called randomforest function to train our model.
library(randomForest)
set.seed(32343)
modelfit_rf <- randomForest(train_data.classe~.,data = train_vif,mtry=6,ntree=2001,importance=TRUE)
# varImpPlot will be plotting significance of different factors.
varImpPlot(modelfit_rf)

# we have used confusion matrix under CARET package to check out of sample error. out of sample accuracy is 0.9973 with using randomforest. 
confusionMatrix(predict(modelfit_rf,test_vif),test_vif$test_data.classe)

```
# Above graph is showing importance of factor for random forest

## prediction with using Grediant Boosting Method

```{r}
set.seed(32343)
modelfit_gbm <- train(train_data.classe~.,data=train_vif,method="gbm") 

# we have used confusion matrix under CARET package to check out of sample error. out of sample accuracy is 0.9973 with using randomforest. 

confusionMatrix(predict(modelfit_gbm,test_vif),test_vif$test_data.classe)

```

## Prediction with using classification method

```{r}
# Train function under CARET is having  
modelfit_class <- train(train_data.classe ~ ., method="rpart", data=train_vif)
# we have installed rattle package to create fancy tree
fancyRpartPlot(modelfit_class$finalModel)


pred_test <- table(predict(modelfit_class, test_vif), test_vif$test_data.classe)
pred_test
# Out of sample accuracy of classification method is 52.63% which is very less than random forest.   
sum(diag(pred_test)) / nrow(test_vif)


```

## Comparison


Random forest Accuracy : 0.9973
Generalize boosting method Accuracy : 0.9888 
Classification method accuracy : 0.5263

Accuracy of random forest is high so final validation will be made using random forest.


## Final prediction of testing data set with using random forest
```{r}
# our train_vif data has reduced to 34 column. So we need to create out testing dataset to sample unique column to predict classes A,B,C,D and E. Following code is reducing testing data set variable from 160 to 34 column.

set.seed(32343)
testing <- testing[,-c(1:6)] 
testing <- testing[,-c(3:30)] 
testing <- testing[,-c(6:9)] 
testing <- testing[,-c(12:21)] 
testing <- testing[,-c(13,15,16,19:35)] 
testing <- testing[,-c(19:33)] 
testing <- testing[,-c(20:29)] 
testing <- testing[,-c(22:26)] 
testing <- testing[,-c(27:41,43:52)] 
testing <- testing[,-c(30:32)] 

# Prediction with using random forest
predict(modelfit_rf,testing)
```
